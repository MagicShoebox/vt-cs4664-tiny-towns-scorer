{"cells":[{"cell_type":"markdown","metadata":{"id":"GMOoN6wxe-Gr"},"source":["[Open in Google Colab](https://colab.research.google.com/github/MagicShoebox/vt-cs4664-tiny-towns-scorer/blob/main/training.ipynb)\n","\n","# Model Training\n","\n","Tiny Towns Scorer\\\n","CS 4664: Data-Centric Computing Capstone\n","\n","### Authors\n","Alex Owens, Daniel Schoenbach, Payton Klemens\n","\n","### Acknowledgements\n","\n","Portions of this project were adapted from tutorials and examples available\n","on the [OpenCV](https://opencv.org/) and [Keras](https://keras.io/) websites.\n","\n","In particular, significant portions of code from the tutorials [Feature Detection and Description](https://docs.opencv.org/4.6.0/db/d27/tutorial_py_table_of_contents_feature2d.html) and [Train an Object Detection Model on Pascal VOC 2007 using KerasCV](https://keras.io/guides/keras_cv/retina_net_overview/) were copied entirely or used as the basis for several cells in this notebook."]},{"cell_type":"markdown","metadata":{"id":"X4K29GqUIDTn"},"source":["# Dependencies"]},{"cell_type":"markdown","metadata":{"id":"cpv08mtwZ7be"},"source":["This notebook uses some additional libraries not installed on Colab by default:\n","- `tensorflow-io` extends TensorFlow with additional features.\\\n","We use it to handle EXIF metadata in images.\n","\n","- `keras-cv` extends Keras with computer vision-focused tools.\\\n","We use the RetinaNet model, bounding box utilities, and more.\n","\n","- `luketils` is used in, and written by the author of, the tutorial referenced above.\\\n","We use some of its visualization functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_C4LOZNzeyo"},"outputs":[],"source":["!pip install tensorflow-io==0.26.0 keras-cv luketils"]},{"cell_type":"markdown","metadata":{"id":"D32sKLNWaA6c"},"source":["Import the major packages we'll be using. Other imports will be done as needed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6G-G_kIRIf8"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_io as tfio\n","from tensorflow import keras\n","import keras_cv\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"zwedq2e8fuoi"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"msUZsBkMNj3p"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"-rvwpylqXHQq"},"source":["Global parameters used in pipeline construction and training.\n","\n","`IMAGE_SIZE` - The square image size for the neural network portion of the model. Images are resized to `IMAGE_SIZE`x`IMAGE_SIZE` without respect for their original aspect ratio. Note that once the network has made predictions, other parts of the model will use the original image.\n","\n","`BATCH_SIZE` - Number of images to process at once.\n","\n","`EPOCHS` - Maximum number of epochs to train."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGQl_vgd_c7p"},"outputs":[],"source":["IMAGE_SIZE = 512\n","BATCH_SIZE = 16\n","EPOCHS = 200"]},{"cell_type":"markdown","metadata":{"id":"VtsY8m1DTpvJ"},"source":["The collected data has been made available on [Zenodo](https://zenodo.org/record/7429657#.Y5d_np7MKUk).\n","\n","The dataset is over 1 GB. To avoid re-downloading it each time, the notebook saves it to Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-qK4JrI8Cj4"},"outputs":[],"source":["# If you do not want to connect the notebook to your Google Drive,\n","# simply uncomment this line and comment the three below\n","# PROJECT_FOLDER = '/content'\n","\n","# Comment to prevent connecting to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","PROJECT_FOLDER = '/content/drive/My Drive/tiny-towns-scorer'\n","\n","from os import path\n","IMAGES_FOLDER = path.join(PROJECT_FOLDER, 'images')\n","ANNOTATIONS_FOLDER = path.join(PROJECT_FOLDER, 'annotations')\n","MODEL_FOLDER = path.join(PROJECT_FOLDER, 'model', 'model')\n","CHECKPOINT_PATH = \"checkpoint/\" # Note: local to runtime environment"]},{"cell_type":"code","source":["!mkdir -p \"{PROJECT_FOLDER}\"\n","!test ! -d \"{IMAGES_FOLDER}\" && wget -O \"images.tar.gz\" \"https://zenodo.org/record/7429657/files/images.tar.gz?download=1\" && tar -xzvf images.tar.gz -C \"{PROJECT_FOLDER}\""],"metadata":{"id":"SLd8gKyKEpDk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GI2qCIaSNMXG"},"source":["We'll also want the list of classes, simply hardcoded here:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zge_f6sb_vGR"},"outputs":[],"source":["class_ids = [\n","  'brick',\n","  'chapel',\n","  'cottage',\n","  'farm',\n","  'tavern',\n","  'theater',\n","  'wheat',\n","  'wood',\n","  'board',\n","  'factory',\n","  'stone',\n","  'well',\n","  'glass',\n","]\n","class_mapping = dict(zip(range(len(class_ids)), class_ids))\n","print(class_mapping)"]},{"cell_type":"markdown","metadata":{"id":"2cbVbAA5MIWX"},"source":["## Parsing"]},{"cell_type":"markdown","metadata":{"id":"7NrzsrZPTxE3"},"source":["Images taken with smartphones often have EXIF Orientation Metadata. CVAT reads this, so the annotations were made using images' correct orientation. However, TensorFlow ignores it by default, so we'll need a function to correct orientation when loading images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzYnIpr7C9bk"},"outputs":[],"source":["# Apply EXIF Orientation to image tensor\n","# Adapted from:\n","# https://medium.com/@ageitgey/the-dumb-reason-your-fancy-computer-vision-app-isnt-working-exif-orientation-73166c7d39da\n","def fix_orientation(img, orientation):\n","  if orientation == 1:\n","    # Normal image - nothing to do!\n","    pass\n","  elif orientation == 2:\n","    # Mirrored left to right\n","    img = tf.image.flip_left_right(img)\n","  elif orientation == 3:\n","    # Rotated 180 degrees\n","    img = tf.image.rot90(img, 2)\n","  elif orientation == 4:\n","    # Mirrored top to bottom\n","    img = tf.image.flip_up_down(img)\n","  elif orientation == 5:\n","    # Mirrored along top-left diagonal\n","    img = tf.image.rot90(img, -1)\n","    img = tf.image.flip_left_right(img)\n","  elif orientation == 6:\n","    # Rotated 90 degrees\n","    img = tf.image.rot90(img, -1)\n","  elif orientation == 7:\n","    # Mirrored along top-right diagonal\n","    img = tf.image.rot90(img, 1)\n","    img = tf.image.flip_left_right(img)\n","  elif orientation == 8:\n","    # Rotated 270 degrees\n","    img = tf.image.rot90(img, 1)\n","  return img"]},{"cell_type":"markdown","metadata":{"id":"8_LDuz-WOkkK"},"source":["The dataset is stored in two parts: raw image files and annotations. The annotations are stored in Tensorflow's TFRecords format. When loading the dataset, we will parse each record, load its associated image, and reformat its annotations for use with KerasCV. While loading the image, we also resize it and correct its orientation using the function above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8o9Kc1jOjvC"},"outputs":[],"source":["from keras_cv import bounding_box\n","\n","# This was inspired by:\n","# https://github.com/keras-team/keras-cv/blob/v0.3.4/keras_cv/datasets/pascal_voc/load.py\n","# This function returns a function that takes a dataset.\n","# The intended usage is dataset.apply( parse_cvat_tfrecords(...) )\n","def parse_cvat_tfrecords(bounding_box_format, img_size=None):\n","\n","  # https://opencv.github.io/cvat/docs/manual/advanced/formats/format-tfrecord/\n","  # Switched VarLenFeature to RaggedFeature\n","  image_feature_description = {\n","      'image/filename': tf.io.FixedLenFeature([], tf.string),\n","      'image/source_id': tf.io.FixedLenFeature([], tf.string),\n","      'image/height': tf.io.FixedLenFeature([], tf.int64),\n","      'image/width': tf.io.FixedLenFeature([], tf.int64),\n","      # Object boxes and classes.\n","      'image/object/bbox/xmin': tf.io.RaggedFeature(tf.float32),\n","      'image/object/bbox/xmax': tf.io.RaggedFeature(tf.float32),\n","      'image/object/bbox/ymin': tf.io.RaggedFeature(tf.float32),\n","      'image/object/bbox/ymax': tf.io.RaggedFeature(tf.float32),\n","      'image/object/class/label': tf.io.RaggedFeature(tf.int64),\n","      'image/object/class/text': tf.io.RaggedFeature(tf.string),\n","  }\n","\n","  # TODO: Use keras-cv resizing layer that respects bounding boxes\n","  # See: https://github.com/keras-team/keras-cv/blob/master/keras_cv/datasets/pascal_voc/load.py\n","  if img_size is not None:\n","    resizing = keras.layers.Resizing(\n","        height=img_size[0], width=img_size[1], crop_to_aspect_ratio=False\n","    )\n","\n","  # Construct function to parse individual record\n","  def parse_record(example_proto):\n","    features = tf.io.parse_example(example_proto, image_feature_description)\n","    filename = tf.strings.join([IMAGES_FOLDER, path.sep, features['image/filename']])\n","    image_raw = tf.io.read_file(filename)\n","    # Not normalizing here due to bug in luketils plot_bounding_box_gallery\n","    # https://github.com/LukeWood/luketils/issues/13\n","    image = tf.io.decode_image(image_raw, channels=3) # / 255\n","    image = tf.ensure_shape(image, [None,None,3])\n","    image = tf.cond(tf.image.is_jpeg(image_raw),\n","                   lambda: fix_orientation(image, tfio.experimental.image.decode_jpeg_exif(image_raw)),\n","                   lambda: image)\n","    if img_size is not None:\n","      image = resizing(image)\n","    bounding_boxes = tf.ragged.stack(\n","        [features['image/object/bbox/ymin'],\n","        features['image/object/bbox/xmin'],\n","        features['image/object/bbox/ymax'],\n","        features['image/object/bbox/xmax'],\n","        tf.cast(features['image/object/class/label'] - 1, tf.float32)],\n","        axis=1\n","        )\n","    bounding_boxes = bounding_box.convert_format(\n","        bounding_boxes,\n","        images=image,\n","        source='rel_yxyx',\n","        target=bounding_box_format\n","    )\n","    return {'images': image, 'bounding_boxes': bounding_boxes}\n","  \n","  # Construct function that applies parse_record to every record in dataset\n","  def apply(dataset):\n","    return dataset.map(parse_record)\n","\n","  # Return that function\n","  return apply"]},{"cell_type":"markdown","metadata":{"id":"FxsV9cPrgILt"},"source":["## Pipelines"]},{"cell_type":"markdown","metadata":{"id":"oHyqX_goUHJ9"},"source":["We are now ready to construct the data pipelines. We begin by creating TensorFlow datasets for the two categories of annotation records: images from the played game and images of individual pieces taken afterward. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ao7fmd8Cxms"},"outputs":[],"source":["# Prepare the game state annotation records\n","game_state_records = tf.data.TFRecordDataset([\n","          path.join(ANNOTATIONS_FOLDER, 'top_down_alex.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'top_down_daniel.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'top_down_payton.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'frontal_alex.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'frontal_daniel.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'frontal_payton.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'side_angle_alex.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'side_angle_daniel.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'side_angle_payton.tfrecord'),\n","])\n","\n","# Prepare the individual piece annotation records\n","piece_records = tf.data.TFRecordDataset([\n","          path.join(ANNOTATIONS_FOLDER, 'pieces_buildings.tfrecord'),\n","          path.join(ANNOTATIONS_FOLDER, 'pieces_resources.tfrecord'),\n","])"]},{"cell_type":"markdown","metadata":{"id":"DfLHygWJSre_"},"source":["The training dataset is constructed by combining the images of individual pieces and 80% of the game state photos. The other 20% of the game state images are used for validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aareVvb7Sq5u"},"outputs":[],"source":["import random\n","\n","# Load and shuffle all the game state records so we can make train/val split\n","game_state_records = list(game_state_records)\n","random.shuffle(game_state_records)\n","\n","# Split game state records 80% Train / 20% Validation\n","all_records = tf.data.Dataset.from_tensor_slices(game_state_records)\n","train_records = tf.data.Dataset.from_tensor_slices(game_state_records[:(len(game_state_records)*4 + 4)//5])\n","val_records = tf.data.Dataset.from_tensor_slices(game_state_records[(len(game_state_records)*4 + 4)//5:])\n","\n","# Add piece records to game state records allocated for training\n","train_records = train_records.concatenate(piece_records)\n","\n","# Shuffle the complete training split\n","train_records = list(train_records)\n","random.shuffle(train_records)\n","train_records = tf.data.Dataset.from_tensor_slices(train_records)"]},{"cell_type":"markdown","metadata":{"id":"xemTKu6EVqt2"},"source":["Lastly, use the functions defined in [Parsing](#parsing) to convert the datasets into a more useful format. Since the dataset is relatively small, we can cache it in memory for performance. We also shuffle and batch the the datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CMijNlWUex0"},"outputs":[],"source":["# Convert annotation records to batches of {'images': img, 'bounding_boxes': boxes}\n","def parse_cache_shuffle_batch(ds):\n","  ds = ds.apply(parse_cvat_tfrecords('xywh', (IMAGE_SIZE, IMAGE_SIZE)))\n","  ds = ds.cache()\n","  ds = ds.shuffle(8 * BATCH_SIZE, reshuffle_each_iteration=True)\n","  ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=BATCH_SIZE))\n","  return ds\n","\n","# Train and validation datasets ready to go\n","all_ds = parse_cache_shuffle_batch(all_records)\n","train_ds = parse_cache_shuffle_batch(train_records)\n","val_ds = parse_cache_shuffle_batch(val_records)"]},{"cell_type":"markdown","metadata":{"id":"r2c7oF6cZ0iX"},"source":["## Augmentation"]},{"cell_type":"markdown","metadata":{"id":"PJVceEOsbGU0"},"source":["Use data augmentation to artificially create more training data. To preserve the annotation bounding boxes, we only perform simple flips and brightness- or color-based distortions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2warhama2sX"},"outputs":[],"source":["random_flip = keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\")\n","rand_augment = keras_cv.layers.RandAugment(\n","    value_range=(0, 255),\n","    augmentations_per_image=2,\n","    # we disable geometric augmentations for object detection tasks\n","    geometric=False,\n",")\n","\n","def augment(inputs):\n","    # In future KerasCV releases, RandAugment will support\n","    # bounding box detection\n","    inputs[\"images\"] = rand_augment(inputs[\"images\"])\n","    inputs = random_flip(inputs)\n","    return inputs\n","\n","augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"Ho9Pi823hpAW"},"source":["Lastly, we'll reformat the pipelines from dictionaries to tuples in preparation for model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zhtnELTfW8q"},"outputs":[],"source":["def dict_to_tuple(inputs):\n","    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n","\n","all_ds = all_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","augmented_ds = augmented_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","all_ds = all_ds.prefetch(tf.data.AUTOTUNE)\n","train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n","augmented_ds = augmented_ds.prefetch(tf.data.AUTOTUNE)\n","val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"OTlWLaSwhjvm"},"source":["# Visualization"]},{"cell_type":"markdown","metadata":{"id":"ILDcxEfVhh7c"},"source":["Define a function to help us visualize datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7GDQ5EHZYGz"},"outputs":[],"source":["from luketils import visualization\n","\n","def visualize_dataset(dataset, bounding_box_format):\n","    images, boxes = next(iter(dataset))\n","    visualization.plot_bounding_box_gallery(\n","        images,\n","        value_range=(0, 255),\n","        bounding_box_format=bounding_box_format,\n","        y_true=boxes,\n","        scale=4,\n","        rows=3,\n","        cols=3,\n","        show=True,\n","        thickness=4,\n","        font_scale=1,\n","        class_mapping=class_mapping,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"uHoQGP7wZulv"},"source":["Visualize a sample of the original training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NU9BI0EZg1n"},"outputs":[],"source":["visualize_dataset(train_ds, bounding_box_format=\"xywh\")"]},{"cell_type":"markdown","metadata":{"id":"ibo8bVLFibkO"},"source":["And a sample of the augmented dataset we'll use for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WkfZFn1h7u-"},"outputs":[],"source":["visualize_dataset(augmented_ds, bounding_box_format=\"xywh\")"]},{"cell_type":"markdown","metadata":{"id":"nE9bbym3cS9b"},"source":["# Model Creation"]},{"cell_type":"markdown","metadata":{"id":"oCBBur-xdAV9"},"source":["Construct a RetinaNet neural network with ResNet50 backbone, pretrained on weights learned from ImageNet. The backbone weights are frozen and not trained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIOhesHxcfbB"},"outputs":[],"source":["model = keras_cv.models.RetinaNet(\n","    # number of classes to be used in box classification\n","    classes=len(class_ids),\n","    # For more info on supported bounding box formats, visit\n","    # https://keras.io/api/keras_cv/bounding_box/\n","    bounding_box_format=\"xywh\",\n","    # KerasCV offers a set of pre-configured backbones\n","    backbone=\"resnet50\",\n","    # Each backbone comes with multiple pre-trained weights\n","    # These weights match the weights available in the `keras_cv.model` class.\n","    backbone_weights=\"imagenet\",\n","    # include_rescaling tells the model whether your input images are in the default\n","    # pixel range (0, 255) or if you have already rescaled your inputs to the range\n","    # (0, 1).  In our case, we feed our model images with inputs in the range (0, 255).\n","    include_rescaling=True,\n","    # Typically, you'll want to set this to False when training a real model.\n","    # evaluate_train_time_metrics=True makes `train_step()` incompatible with TPU,\n","    # and also causes a massive performance hit.  It can, however be useful to produce\n","    # train time metrics when debugging your model training pipeline.\n","    evaluate_train_time_metrics=False,\n",")\n","# Fine-tuning a RetinaNet is as simple as setting backbone.trainable = False\n","model.backbone.trainable = False"]},{"cell_type":"markdown","metadata":{"id":"8D7R5XzVeUhY"},"source":["Before we compile the model, we define evaluation metrics. We use the COCO Metrics provided by KerasCV. Since we want COCO Metrics for all classes, as well as each individual class, we define a couple helper functions first."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20Cb7J3ceuQD"},"outputs":[],"source":["def coco_map_metric(name, class_ids):\n","  return keras_cv.metrics.COCOMeanAveragePrecision(\n","            class_ids=class_ids,\n","            bounding_box_format=\"xywh\",\n","            name=name)\n","def coco_recall_metric(name, class_ids):\n","  return keras_cv.metrics.COCORecall(\n","            class_ids=class_ids,\n","            bounding_box_format=\"xywh\",\n","            max_detections=100,\n","            name=name)"]},{"cell_type":"markdown","metadata":{"id":"S5_ShTGje0gI"},"source":["Equip the model with RetinaNet's \"focal loss\" function and prepare it for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkiwBPJSZ0Au"},"outputs":[],"source":["model.compile(\n","    classification_loss=keras_cv.losses.FocalLoss(from_logits=True, reduction=\"none\"),\n","    box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction=\"none\"),\n","    optimizer=tf.optimizers.SGD(global_clipnorm=10.0),\n","    metrics=[\n","        coco_map_metric(\"Total Mean Average Precision\", range(len(class_ids))),\n","        coco_recall_metric(\"Total Recall\", range(len(class_ids))),\n","        *(coco_map_metric(f'{cid} Mean Average Precision', [idx]) for idx, cid in enumerate(class_ids)),\n","        *(coco_recall_metric(f'{cid} Recall', [idx]) for idx, cid in enumerate(class_ids))\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"O1byR4ZpfJe7"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"wS3QtMo_fRs5"},"source":["Train the model, with some callbacks to adjust the learning rate as needed and to allow for early stopping. Weights will only be saved **locally** after training is complete.\n","\n","**We recommend using a GPU hardware accelerator.**\n","\n","**Running this cell may take an hour or more.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0fGSl-ofwvB"},"outputs":[],"source":["model.fit(\n","  augmented_ds,\n","  validation_data=val_ds.take(20),\n","  epochs=EPOCHS,\n","  callbacks=[\n","    keras.callbacks.TensorBoard(log_dir=\"logs\"),\n","    keras.callbacks.ReduceLROnPlateau(patience=5),\n","    keras.callbacks.EarlyStopping(patience=10),\n","    keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, save_weights_only=True),\n","  ],\n",")\n","model.save_weights(CHECKPOINT_PATH)"]},{"cell_type":"markdown","metadata":{"id":"6AnDy-sojzmD"},"source":["# Model Evaluation"]},{"cell_type":"markdown","metadata":{"id":"_rIlufYDkS4E"},"source":["Now we can evaluate our COCO Metrics on the trained model. We evaluate both against the validation dataset, as well as the dataset of all game state photos. (Individual piece photos are only used for training.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cw4usRPvfbR8"},"outputs":[],"source":["model.load_weights(CHECKPOINT_PATH)\n","for name, ds in [('All dataset', all_ds.take(100)),('Validation dataset', val_ds.take(100))]:\n","  metrics = model.evaluate(ds, return_dict=True)\n","  print(f'{name} metrics:')\n","  print(metrics)"]},{"cell_type":"markdown","metadata":{"id":"KEA-o1xVj3at"},"source":["# Saving the Model"]},{"cell_type":"markdown","metadata":{"id":"QqEALQkyleb2"},"source":["If the `MODEL_FOLDER` set earlier is on a Google Drive, save the weights there for use in the other notebooks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Pog_4Ddla7b"},"outputs":[],"source":["model.save_weights(MODEL_FOLDER)"]},{"cell_type":"markdown","metadata":{"id":"gWGVZVBelqyw"},"source":["If you want to do \"fire and forget\" training, you can uncomment the last line, then use Run All to disconnect the runtime at the end of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIEieryplqfe"},"outputs":[],"source":["from google.colab import runtime\n","\n","# Make sure we finished saving\n","drive.flush_and_unmount()\n","\n","# Uncomment to have Run All terminate the session when done\n","# runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["X4K29GqUIDTn"],"provenance":[{"file_id":"1k0fEoLL6SnUIfJFPj4QeU8qG4Z2utH6I","timestamp":1670779312880},{"file_id":"https://github.com/keras-team/keras-io/blob/master/guides/ipynb/keras_cv/retina_net_overview.ipynb","timestamp":1668181777081}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}