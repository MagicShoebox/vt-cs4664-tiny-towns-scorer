{"cells":[{"cell_type":"markdown","metadata":{"id":"GMOoN6wxe-Gr"},"source":["[Open in Google Colab](https://github.com/MagicShoebox/vt-cs4664-tiny-towns-scorer/blob/main/scorer.ipynb)\n","\n","# Scorer\n","\n","Tiny Towns Scorer\\\n","CS 4664: Data-Centric Computing Capstone\n","\n","### Authors\n","Alex Owens, Daniel Schoenbach, Payton Klemens\n","\n","### Acknowledgements\n","\n","Portions of this project were adapted from tutorials and examples available\n","on the [OpenCV](https://opencv.org/) and [Keras](https://keras.io/) websites.\n","\n","In particular, significant portions of code from the tutorials [Feature Detection and Description](https://docs.opencv.org/4.6.0/db/d27/tutorial_py_table_of_contents_feature2d.html) and [Train an Object Detection Model on Pascal VOC 2007 using KerasCV](https://keras.io/guides/keras_cv/retina_net_overview/) were copied entirely or used as the basis for several cells in this notebook."]},{"cell_type":"markdown","metadata":{"id":"X4K29GqUIDTn"},"source":["# Dependencies"]},{"cell_type":"markdown","metadata":{"id":"cpv08mtwZ7be"},"source":["This notebook uses some additional libraries not installed on Colab by default:\n","- `keras-cv` extends Keras with computer vision-focused tools.\\\n","We use the RetinaNet model, bounding box utilities, and more.\n","\n","- `luketils` is used in, and written by the author of, the tutorial referenced above.\\\n","We use some of its visualization functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_C4LOZNzeyo"},"outputs":[],"source":["!pip install keras-cv luketils"]},{"cell_type":"markdown","metadata":{"id":"D32sKLNWaA6c"},"source":["Import the major packages we'll be using. Other imports will be done as needed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6G-G_kIRIf8"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import keras_cv\n","import matplotlib.pyplot as plt\n","import cv2 as cv\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"msUZsBkMNj3p"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"-rvwpylqXHQq"},"source":["Global parameters that were used to construct the network portion of the model.\n","\n","`IMAGE_SIZE` - The square image size for the neural network portion of the model. Images are resized to `IMAGE_SIZE`x`IMAGE_SIZE` without respect for their original aspect ratio. Note that once the network has made predictions, other parts of the model will use the original image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGQl_vgd_c7p"},"outputs":[],"source":["IMAGE_SIZE = 512"]},{"cell_type":"markdown","metadata":{"id":"VtsY8m1DTpvJ"},"source":["The collected data has been made available on [Zenodo](https://zenodo.org/record/7429657#.Y5d_np7MKUk).\n","\n","The dataset is over 1 GB. To avoid re-downloading it each time, the notebook saves it to Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-qK4JrI8Cj4"},"outputs":[],"source":["# If you do not want to connect the notebook to your Google Drive,\n","# simply uncomment this line and comment the three below\n","# PROJECT_FOLDER = '/content'\n","\n","# Comment to prevent connecting to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","PROJECT_FOLDER = '/content/drive/My Drive/tiny-towns-scorer'\n","\n","from os import path\n","IMAGES_FOLDER = path.join(PROJECT_FOLDER, 'images')\n","MODEL_FOLDER = path.join(PROJECT_FOLDER, 'model', 'model')"]},{"cell_type":"code","source":["!mkdir -p \"{PROJECT_FOLDER}\"\n","!test ! -d \"{IMAGES_FOLDER}\" && wget -O \"images.tar.gz\" \"https://zenodo.org/record/7429657/files/images.tar.gz?download=1\" && tar -xzvf images.tar.gz -C \"{PROJECT_FOLDER}\"\n","!test ! -d \"{MODEL_FOLDER}\" && wget -O \"model.tar.gz\" \"https://zenodo.org/record/7429657/files/model.tar.gz?download=1\" && tar -xzvf model.tar.gz -C \"{PROJECT_FOLDER}\""],"metadata":{"id":"iOC31lCVShJc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GI2qCIaSNMXG"},"source":["We'll also want the list of classes, simply hardcoded here:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zge_f6sb_vGR"},"outputs":[],"source":["class_ids = [\n","  'brick',\n","  'chapel',\n","  'cottage',\n","  'farm',\n","  'tavern',\n","  'theater',\n","  'wheat',\n","  'wood',\n","  'board',\n","  'factory',\n","  'stone',\n","  'well',\n","  'glass',\n","]\n","class_mapping = dict(zip(range(len(class_ids)), class_ids))\n","print(class_mapping)"]},{"cell_type":"markdown","metadata":{"id":"OTlWLaSwhjvm"},"source":["# Visualization"]},{"cell_type":"markdown","metadata":{"id":"ILDcxEfVhh7c"},"source":["Define a function to help us visualize detections."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7GDQ5EHZYGz"},"outputs":[],"source":["from luketils import visualization\n","\n","def visualize_detections(image, predictions):\n","  visualization.plot_bounding_box_gallery(\n","          [image],\n","          value_range=(0, 255),\n","          bounding_box_format='xywh',\n","          y_true=None,\n","          y_pred=[predictions],\n","          pred_color=(128,255,128),\n","          scale=12,\n","          rows=1,\n","          cols=1,\n","          show=True,\n","          thickness=12,\n","          font_scale=4,\n","          class_mapping=class_mapping,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"nE9bbym3cS9b"},"source":["# Model Creation"]},{"cell_type":"markdown","metadata":{"id":"8zGAv9MpVSV9"},"source":["## RetinaNet"]},{"cell_type":"markdown","metadata":{"id":"oCBBur-xdAV9"},"source":["Reconstruct the RetinaNet neural network created in `training.ipynb`. The networks must match in order to load the saved weights. There is no need to load the ImageNet weights, since they were included in the saved weights.\n","\n","(In an application environment, model creation could be extracted into a shared module. Alternatively, Keras has tools for serializing the model structure in addition to the weights. In this setting, however, duplicating the code is much simpler and suffices for now.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIOhesHxcfbB"},"outputs":[],"source":["model = keras_cv.models.RetinaNet(\n","    # number of classes to be used in box classification\n","    classes=len(class_ids),\n","    # For more info on supported bounding box formats, visit\n","    # https://keras.io/api/keras_cv/bounding_box/\n","    bounding_box_format=\"xywh\",\n","    # KerasCV offers a set of pre-configured backbones\n","    backbone=\"resnet50\",\n","    # include_rescaling tells the model whether your input images are in the default\n","    # pixel range (0, 255) or if you have already rescaled your inputs to the range\n","    # (0, 1).  In our case, we feed our model images with inputs in the range (0, 255).\n","    include_rescaling=True,\n","    # Typically, you'll want to set this to False when training a real model.\n","    # evaluate_train_time_metrics=True makes `train_step()` incompatible with TPU,\n","    # and also causes a massive performance hit.  It can, however be useful to produce\n","    # train time metrics when debugging your model training pipeline.\n","    evaluate_train_time_metrics=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"EEwO_TpRXinE"},"source":["Load the weights saved to `MODEL_FOLDER` when `training.ipynb` was run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMsxEUm6XZ68"},"outputs":[],"source":["model.load_weights(MODEL_FOLDER)"]},{"cell_type":"markdown","metadata":{"id":"uocZrec6Xn4n"},"source":["Configure the network's prediction decoder, then wrap the network in a function that resizes an input image, gets the predictions, and then returns predictions resized to the original image's proportions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RpJ1yA7XW7x"},"outputs":[],"source":["from keras_cv import bounding_box\n","\n","model.prediction_decoder = keras_cv.layers.NmsPredictionDecoder(\n","    bounding_box_format=\"xywh\",\n","    anchor_generator=keras_cv.models.RetinaNet.default_anchor_generator(\n","        bounding_box_format=\"xywh\"\n","    ),\n","    suppression_layer=keras_cv.layers.NonMaxSuppression(\n","        iou_threshold=0.25, # 0.25?\n","        bounding_box_format=\"xywh\",\n","        classes=len(class_ids),\n","        confidence_threshold=0.33, # 0.6?\n","    ),\n",")\n","\n","# Get the network's object predictions for an image\n","# Each prediction has the format:\n","# [x, y, width, height, class_id, confidence]\n","def get_predictions(image):\n","  img = cv.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","  preds = model.predict(np.array([img]))[0]\n","  rel_pred = bounding_box.convert_format(preds, 'xywh', 'rel_xyxy', img)\n","  preds = bounding_box.convert_format(rel_pred, 'rel_xyxy', 'xywh', image)\n","  return preds.numpy()"]},{"cell_type":"markdown","metadata":{"id":"Jmf8nHeaY5kz"},"source":["## ORB"]},{"cell_type":"markdown","metadata":{"id":"KFXN8ZKWY933"},"source":["Load the reference scan of the game board and compute its features using the ORB algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkiwBPJSZ0Au"},"outputs":[],"source":["board_ref_file = path.join(IMAGES_FOLDER, 'boards/board_scan_transparent.png')\n","board_ref_img = cv.cvtColor(cv.imread(board_ref_file), cv.COLOR_BGR2RGB)\n","\n","orb = cv.ORB_create()\n","orb.setMaxFeatures(5000)\n","board_ref_kp, board_ref_des = orb.detectAndCompute(board_ref_img,None)"]},{"cell_type":"markdown","metadata":{"id":"esyfxkOqZTqq"},"source":["Define a function that takes an image of a board, computes its features using ORB, and then matches those features against the reference features. If enough matches are found, the function uses homography to compute a projective transformation from the perspective of the input board to the perspective of the reference board. Otherwise, it returns `None`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFU8sm9nZUOf"},"outputs":[],"source":["# Creates projective transformation from input board perspective to reference board perspective\n","def find_board_homography(board_img, verbose=False):\n","  board_kp, board_des = orb.detectAndCompute(board_img,None)\n","  bf = cv.BFMatcher(normType=cv.NORM_HAMMING)\n","  matches = bf.knnMatch(board_ref_des,board_des,k=2)\n","  good = [m for m,n in matches if m.distance < 0.7*n.distance]\n","  if len(good) < 4:\n","    return None\n","  src_pts = np.float32([board_ref_kp[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n","  dst_pts = np.float32([board_kp[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n","  M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)\n","  if M is None:\n","    return None\n","  if verbose:\n","    draw_params = {'matchColor': (128,255,128),\n","                  'singlePointColor': None,\n","                  # 'matchesMask': mask.ravel().tolist(),\n","                  'flags': cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS | cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS}\n","    fig,ax = plt.subplots(figsize=(12,12))\n","    ax.axis('off')\n","    ax.imshow(cv.drawMatches(board_ref_img,board_ref_kp,board_img,board_kp,good,None,**draw_params))\n","    plt.show()\n","  return np.linalg.inv(M)"]},{"cell_type":"markdown","metadata":{"id":"4Bp3a3Vha6hj"},"source":["## Utilities"]},{"cell_type":"markdown","metadata":{"id":"r9vOzgiVa73V"},"source":["A series of utility functions for manipulating images and bounding boxes. These will be used in the next section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFYfrbGebplg"},"outputs":[],"source":["from math import sqrt\n","from itertools import filterfalse, tee\n","\n","# Combination of min and max.\n","# Give x, returns x such that mn <= x <= mx.\n","def clamp(x, mn, mx):\n","  return mn if x < mn else (mx if x > mx else x)\n","\n","# https://docs.python.org/dev/library/itertools.html#itertools-recipes\n","# Splits an iterable according to a predicate.\n","def partition(pred, iterable):\n","    \"Use a predicate to partition entries into false entries and true entries\"\n","    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\n","    t1, t2 = tee(iterable)\n","    return filterfalse(pred, t1), filter(pred, t2)\n","\n","# Increases the area of a prediction box by a factor.\n","# The expanded box is clipped to stay inside (0,0) and maxdim.\n","def expand(box, factor, maxdim):\n","  xmin, ymin, width, height = box[:4]\n","  sqrtf = sqrt(factor)\n","  xmin = max(0, xmin - width*(sqrtf-1)/2)\n","  ymin = max(0, ymin - height*(sqrtf-1)/2)\n","  width = min(maxdim[1]-xmin, width * sqrtf)\n","  height = min(maxdim[0]-ymin, height * sqrtf)\n","  return [xmin, ymin, width, height, *box[4:]]\n","\n","# Converts a prediction box to a prediction point.\n","def center(box):\n","  return [box[0]+box[2]/2, box[1]+box[3]/2, *box[4:]]\n","\n","# Given a prediction box, returns a predicate\n","# that tests if a point is inside that box.\n","def inside(board_pred):\n","  xmin, ymin, width, height = board_pred[:4]\n","  def apply(pt):\n","    x,y = pt[:2]\n","    return xmin <= x <= xmin+width and ymin <= y <= ymin + height\n","  return apply\n","\n","# Given a prediction box, returns a function\n","# that translates point coordinates to be relative to the box.\n","def translate(board_pred):\n","  xmin, ymin = board_pred[:2]\n","  def apply(pt):\n","    x,y = pt[:2]\n","    return [x-xmin, y-ymin, *pt[2:]]\n","  return apply\n","\n","# Given a prediction box, returns a function\n","# that translates point coordinates to be relative to the box.\n","def translate(board_pred):\n","  xmin, ymin = board_pred[:2]\n","  def apply(pt):\n","    x,y = pt[:2]\n","    return [x-xmin, y-ymin, *pt[2:]]\n","  return apply\n","\n","# Given an image and prediction box,\n","# returns the portion of the image contained by the box.\n","def extract(image, box):\n","  xmin = round(box[0])\n","  ymin = round(box[1])\n","  xmax = round(box[0]+box[2])\n","  ymax = round(box[1]+box[3])\n","  return image[ymin:ymax,xmin:xmax]\n","\n","# Given an area, returns a function\n","# that classifies points in that area into a 4x4 grid.\n","def gridify(width, height):\n","  def apply(pt):\n","    x,y = pt[:2]\n","    return [clamp(int(x // (width/4)),0,3), clamp(int(y // (height/4)),0,3), *pt[2:]]\n","  return apply"]},{"cell_type":"markdown","metadata":{"id":"mK-d-o-itgFX"},"source":["## Get Game State"]},{"cell_type":"markdown","metadata":{"id":"ZFxtMSQmtxjX"},"source":["Combine all the functions into a single model that takes an image and returns the predicted grid of objects. If `verbose` is `True`, the function also returns the input image transformed into the reference perspective."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoVrOMzctqnL"},"outputs":[],"source":["from operator import itemgetter\n","\n","# Given an image, get its predicted game state.\n","def get_game_state(image, verbose=False):\n","\n","  # Get object predictions from neural network\n","  predictions = get_predictions(image)\n","  if verbose:\n","    visualize_detections(image, predictions)\n","  piece_preds, board_preds = partition(lambda b: class_mapping[b[4]] == 'board', predictions)\n","\n","  # Select highest-confidence board prediction\n","  board_pred = next(iter(sorted(board_preds, key=itemgetter(5), reverse=True)), None)\n","  if board_pred is None:\n","    raise ValueError('Board could not be found')\n","\n","  # Isolate the board area\n","  board_pred = expand(board_pred, 1.25, image.shape[:2])\n","  board_img  = extract(image, board_pred)\n","\n","  # Use board area to compute projective transformation to reference perspective\n","  homography = find_board_homography(board_img, verbose)\n","  if homography is None:\n","    raise ValueError('Board edges could not be found')\n","\n","  # Replace piece boxes with center points\n","  piece_preds = map(center, piece_preds)\n","  \n","  # Get rid of piece predictions outside of board\n","  piece_preds = filter(inside(board_pred), piece_preds)\n","  \n","  # Shift center points to use board as origin\n","  piece_preds = map(translate(board_pred), piece_preds)\n","  \n","  # Make it a list so we can check if it's empty or not\n","  piece_preds = list(piece_preds)\n","\n","  # Transform piece center points to reference perspective\n","  if piece_preds:\n","    transformed = cv.perspectiveTransform(np.float32([p[:2] for p in piece_preds]).reshape(-1,1,2), homography)\n","    transformed = map(itemgetter(0), transformed)\n","    piece_preds = [[t[0], t[1], *p[2:]] for t,p in zip(transformed, piece_preds)]\n","\n","  height, width = board_ref_img.shape[:2]\n","\n","  if verbose:\n","    warped = cv.warpPerspective(board_img, homography, (height, width))\n","    fig, ax = plt.subplots(1,1,figsize=(8,8))\n","    ax.axis('off')\n","    ax.imshow(warped)\n","    ax.scatter([p[0] for p in piece_preds], [p[1] for p in piece_preds], s=70, c='#80ff80')\n","    ax.vlines([x*(width/4) for x in range(1,4)], 0, height, colors='#80ff80', linewidth=3)\n","    ax.hlines([y*(height/4) for y in range(1,4)], 0, width, colors='#80ff80', linewidth=3)\n","    plt.show()\n","\n","  # Partition piece predictions into grid,\n","  # keeping only the highest-confidence one in each spot.\n","  piece_preds = map(gridify(width, height), piece_preds)\n","  grid = [[(None, 0) for _ in range(4)] for _ in range(4)]\n","  for p in piece_preds:\n","    cls, conf = grid[p[1]][p[0]]\n","    if p[3] > conf:\n","      grid[p[1]][p[0]] = (p[2], p[3])\n","  \n","  # Return either the grid, or the grid and transformed image.\n","  if verbose:\n","    return grid, warped\n","  return grid"]},{"cell_type":"markdown","metadata":{"id":"qs2R-vHyx6fY"},"source":["## Testing Function"]},{"cell_type":"markdown","metadata":{"id":"bsmWlXt0x8Kz"},"source":["Define a testing function to call the model on any image and report its results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Pvic6Nax_Hz"},"outputs":[],"source":["from matplotlib import patheffects\n","\n","def test_image(file, verbose=False):\n","  image = cv.cvtColor(cv.imread(file), cv.COLOR_BGR2RGB)\n","  fig, ax = plt.subplots(1,1,figsize=(10,10))\n","  ax.axis('off')\n","  ax.imshow(image)\n","  plt.show()\n","  if verbose:\n","    grid, warped = get_game_state(image, verbose)\n","  else:\n","    grid = get_game_state(image, verbose)\n","  for y in range(4):\n","    cell = lambda p: 'None' if p[0] is None else f'{class_mapping[p[0]]}-{p[1]:.2}'\n","    line = ''.join(f'{cell(grid[y][x]):^15}' for x in range(4))\n","    print(line)\n","  \n","  if verbose:\n","    fig, ax = plt.subplots(1,1,figsize=(8,8))\n","    ax.axis('off')\n","    ax.imshow(warped)\n","    for y in range(4):\n","      for x in range(4):\n","        height, width = warped.shape[:2]\n","        x_pos, y_pos = (x+0.4) * width / 4.5, (y+0.5) * height / 4.5\n","        ax.annotate(\n","            class_mapping.get(grid[y][x][0]),\n","            (x_pos, y_pos),\n","            fontsize=18,\n","            weight='bold',\n","            c='#80ffff',\n","            path_effects=[patheffects.withStroke(linewidth=2, foreground=\"b\")])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jYMxwte5xK9z"},"source":["# Try It Out"]},{"cell_type":"markdown","metadata":{"id":"4CYW_76PyZt1"},"source":["These are the 10 images in the dataset that were taken at the conclusion of the played game."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2R07qsSQycVY"},"outputs":[],"source":["final_game_states = [\n","  'frontal/IMG_0278.jpeg', \n","  'frontal/IMG_4556.JPG', \n","  'frontal/IMG_6211.jpg', \n","  'side_angle/IMG_0280.jpeg', \n","  'side_angle/IMG_4557.JPG', \n","  'side_angle/IMG_4558.JPG', \n","  'side_angle/IMG_6212.jpg', \n","  'top_down/IMG_0279.jpeg',\n","  'top_down/IMG_4555.JPG',\n","  'top_down/IMG_6210.jpg'\n","]"]},{"cell_type":"markdown","metadata":{"id":"AweHOEntzAf-"},"source":["We can test the model on these or any of the image files and see how it does."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQxGtS2syccs"},"outputs":[],"source":["image_file = final_game_states[5]\n","test_image(path.join(IMAGES_FOLDER, image_file), verbose=True)"]}],"metadata":{"colab":{"collapsed_sections":["OTlWLaSwhjvm"],"provenance":[{"file_id":"1k0fEoLL6SnUIfJFPj4QeU8qG4Z2utH6I","timestamp":1670779312880},{"file_id":"https://github.com/keras-team/keras-io/blob/master/guides/ipynb/keras_cv/retina_net_overview.ipynb","timestamp":1668181777081}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}